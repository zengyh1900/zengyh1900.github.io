
<tr >
    <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
      <div class="one">
        <img src='images/cherrypicker.png' width=80%>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://yichengchen24.github.io/projects/autocherrypicker/">
        <span class="papertitle">
          Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language
        </span>
      </a>
      <br>
      <a href="https://yichengchen24.github.io/">Yicheng Cheng</a>,
      <a href="https://lxtgh.github.io/">Xiangtai Li</a>,
      <a href="https://liyn.site/">Yining Li</a>,
      <strong>Yanhong Zeng</strong>,
      <a href="https://jianzongwu.github.io/">Jianzong Wu</a>,
      <a href="https://scholar.google.com/citations?hl=zh-CN&user=eqFr7IgAAAAJ">Xiangyu Zhao</a>,
      <a href="https://chenkai.site/">Kai Chen</a>
      <br>
      <em>CVPR</em>, 2025
      <br>
      <a href="https://yichengchen24.github.io/projects/autocherrypicker/">project page</a> /
      <a href="https://arxiv.org/abs/2406.20085">arXiv</a> /
      <a href="https://github.com/yichengchen24/ACP">code</a> /
      <img src="https://img.shields.io/github/stars/yichengchen24/ACP?style=social">
      <p>
        Auto Cherry-Picker is designed to synthesize training samples for both perception and multi-modal reasoning tasks from a simple object list in natural language. It employs a nowly designed metric, CLIS, to ensure the quality of the synthetic data.
      </p>
    </td>
  </tr>



  <tr >
    <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
      <div class="one">
        <img src='images/motionbooth.gif' width=80%>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://jianzongwu.github.io/projects/motionbooth/">
        <span class="papertitle">MotionBooth: Motion-Aware Customized Text-to-Video Generation</span>
      </a>
      <br>
      <a href="https://jianzongwu.github.io/">Jianzong Wu</a>,
      <a href="https://lxtgh.github.io/">Xiangtai Li</a>,
      <strong>Yanhong Zeng</strong>,
      <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
      <a href="https://qianyuzqy.github.io/">Qianyu Zhou</a>,
      <a href="https://liyn.site/">Yining Li</a>,
      <a href="https://scholar.google.com/citations?user=T4gqdPkAAAAJ">Yunhai Tong</a>,
      <a href="https://chenkai.site/">Kai Chen</a>
      <br>
      <em>NeurIPS</em>, 2024 (Spotlight)
      <br>
      <a href="https://jianzongwu.github.io/projects/motionbooth/">project page</a> /
      <a href="https://youtu.be/iuH5iqLk5VQ">video</a> /
      <a href="https://youtu.be/iuH5iqLk5VQ">arXiv</a> /
      <a href="https://github.com/jianzongwu/MotionBooth">code</a>
      <img src="https://img.shields.io/github/stars/jianzongwu/MotionBooth?style=social">
      <p>
        MotionBooth is designed for animating customized subjects with precise control over both object and camera movements.
      </p>
    </td>
  </tr>

<tr >
    <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
      <div class="one">
        <img src='images/humanvid.gif' width=80%>
      </div>
    </td>
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://humanvid.github.io/">
        <span class="papertitle">HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</span>
      </a>
      <br>
      <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>,
      <a href="https://yixuanli98.github.io/">Yixuan Li</a>,
      <strong>Yanhong Zeng</strong>,
      <a href="https://guoyww.github.io/">Yuwei Guo</a>,
      <a href="">Youqing Fang</a>,
      <a href="">Wenran Liu</a>,
      <a href="https://sparkstj.github.io/">Jing Tan</a>,
      <a href="https://chenkai.site/">Kai Chen</a>,
      <a href="https://tianfan.info/">Tianfan Xue</a>,
      <a href="https://daibo.info/">Bo Dai</a>,
      <a href="Bo Dai ">Dahua Lin</a>
      <br>
      <em>NeurIPS</em>, 2024 (D&B Track)
      <br>
      <a href="https://humanvid.github.io/">project page</a> /
      <a href="https://arxiv.org/abs/2407.17438">arXiv</a> /
      <a href="https://github.com/zhenzhiwang/HumanVid">code</a>
      <img src="https://img.shields.io/github/stars/zhenzhiwang/HumanVid?style=social">
      <p>
        HumanVid is the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data.
      </p>
    </td>
  </tr>

  <tr >
    <tr >
      <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/styleshot.png' width=80%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://styleshot.github.io/">
          <span class="papertitle">StyleShot: A SnapShot on Any Style</span>
        </a>
        <br>
        <a href="https://jeoyal.github.io/home/">Junyao Guo</a>,
        <a href="">Yanchen Liu</a>,
        <a href="https://scholar.google.com/citations?hl=zh-CN&user=6TA1oPkAAAAJ">Yanan Sun</a>,
        <a href="">Yinhao Tang</a>,
        <strong>Yanhong Zeng</strong>,
        <a href="https://chenkai.site/">Kai Chen</a>,
        <a href="https://vill-lab.github.io/">Cairong Zhao</a>,
        <br>
        <em>Arxiv</em>, 2024
        <br>
        <a href="https://styleshot.github.io/">project page</a> /
        <a href="https://www.youtube.com/watch?v=FC_LgSJD-lA&t=4s">video</a> /
        <a href="http://arxiv.org/abs/2407.01414">arXiv</a> /
        <a href="https://openxlab.org.cn/apps/detail/lianchen/StyleShot">demo</a> /
        <a href="https://github.com/open-mmlab/StyleShot">code</a>
        <img src="https://img.shields.io/github/stars/open-mmlab/styleshot?style=social">
        <p>
          StyleShot is a style transfer model that excels in text and image-driven style transferring without test-time style-tuning.
        </p>
      </td>
    </tr>


<tr >
    <tr >
        <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
          <div class="one">
            <img src='images/make-it-vivid.gif' width=80%>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://make-it-vivid.github.io/">
            <span class="papertitle">Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text</span>
          </a>
          <br>
          <a href="https://junshutang.github.io/">Junshu Tang</a>,
          <strong>Yanhong Zeng</strong>,
          <a href="">Ke Fan</a>,
          <a href="">Xuheng Wang</a>,
          <a href="https://daibo.info/">Bo Dai</a>,
          <a href="https://scholar.google.com/citations?user=yd58y_0AAAAJ&hl=en">Lizhuang Ma</a>,
          <a href="https://chenkai.site/">Kai Chen</a>
          <br>
          <em>CVPR</em>, 2024
          <br>
          <a href="https://make-it-vivid.github.io/">project page</a> /
          <a href="https://www.youtube.com/watch?v=WQ5QizLucbM">video</a> /
          <a href="https://arxiv.org/abs/2403.16897">arXiv</a> /
          <a href="https://github.com/junshutang/Make-It-Vivid">code</a>
          <img src="https://img.shields.io/github/stars/junshutang/Make-It-Vivid?style=social">
          <p>
          We present Make-it-Vivid, the first attempt that can create plausible and consistent texture in UV space for 3D biped cartoon characters from text input within few seconds.
          </p>
        </td>
      </tr>


<tr>
    <tr>
      <td style="padding:10px;width:20%;vertical-align:middle">
        <a href="images/3dhumanbody.png"><img src="images/3dhumanbody.png" width="80%"></a>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/1900zyh/3dhumanbody?authuser=0">
          <papertitle>3D Human Body Reshaping with Anthropometric Modeling</papertitle>
        </a>
        <br>
        <strong>Yanhong Zeng</strong>,
        <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>,
        <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>
        <br>
        <em>ICIMCS</em>, 2017
        <br>
        <a href="https://sites.google.com/view/1900zyh/3dhumanbody?authuser=0">project page</a> /
        <a href="https://arxiv.org/abs/2104.01762">arXiv</a> /
        <a href="https://www.youtube.com/watch?v=s0GvkER-Y24">video</a> /
        <a href="https://github.com/zengyh1900/3D-Human-Body-Shape">code</a>
        <img src="https://img.shields.io/github/stars/zengyh1900/3D-Human-Body-Shape?style=social">
        <p>We design a 3D human body reshaping system. It can take as input user's anthropometric measurements (e.g., height and weight) and generate a 3D human shape for the user.</p>
      </td>
    </tr>


  <tr >
    <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
      <div class="one">
        <img src='images/DiffSensei.png' width=80%>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://jianzongwu.github.io/projects/diffsensei/">
        <span class="papertitle">
            DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation
        </span>
      </a>
      <br>
      <a href="https://jianzongwu.github.io/">Jianzong Wu</a>,
      <a href="">Chao Tang</a>,
      <a href="https://wangjingbo1219.github.io/">Jingbo Wang</a>,
      <strong>Yanhong Zeng</strong>,
      <a href="https://lxtgh.github.io/">Xiangtai Li</a>,
      <a href="https://scholar.google.com/citations?user=T4gqdPkAAAAJ">Yunhai Tong</a>
      <br>
      <em>CVPR</em>, 2025
      <br>
      <a href="https://jianzongwu.github.io/projects/diffsensei/">project page</a> /
      <a href="https://arxiv.org/abs/2412.07589">arXiv</a> /
      <a href="https://huggingface.co/datasets/jianzongwu/MangaZero">dataset</a> /
      <a href="https://jianzongwu.github.io/projects/diffsensei/static/pdfs/nobel_prize.pdf">demo</a> /
      <a href="https://github.com/jianzongwu/DiffSensei">code</a>
      <img src="https://img.shields.io/github/stars/jianzongwu/DiffSensei?style=social">
      <p>
        <strong>MangaZero</strong> is a new large-scale manga dataset containing 43K manga pages and 427K annotated panels.
        <strong>DiffSensei</strong> is the first model that can generate manga images with high-quality and controllable multiple characters with complex scenes.
      </p>
    </td>
  </tr>