<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


    <title>Yanhong Zeng</title>

    <meta name="author" content="Yanhong Zeng">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <!-- ============================ Bio ==================================== -->
  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yanhong Zeng æ›¾è‰³çº¢
                </p>
                <p>
                  Yanhong Zeng is currently a researcher working on Generative AI in <a href="https://www.antresearch.com/">Ant Group</a>, working with <a href="https://shenyujun.github.io/">Yujun Shen</a>.
                    Before that, she led a small effort in advancing image/video generation in <a href="https://www.shlab.org.cn/">Shanghai AI Lab</a>, working with <a href="https://chenkai.site/">Kai Chen</a>.
                    She obtained her computer science Ph.D. degree in the joint doctoral program between <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a>
                 and <a href="https://www.msra.cn/">Microsoft Research Asia (MSRA)</a> in 2022, working with <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
                 supervised by <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Prof. Hongyang Chao</a>
                 and <a href="https://www.microsoft.com/en-us/research/people/bainguo/">Dr. Baining Guo</a>.
                </p>
                <p>
                  ðŸ’— Her research interest is <span style="background-color: #fff3cd; padding: 2px 5px; border-radius: 3px;">advancing high-quality and controllable generative models and systems across media including images, videos, and audio.</span> Her passion lies in democratizing creativity by transforming ideas into compelling and shareable contents.
                </p>

                <p>
                  <div style="background-color: #e7f3fe; border-left: 6px solid #2196F3; padding: 15px; margin: 15px 0;">
                    <p style="margin: 0; color: #ff0000;">
                      ðŸ’— Happy to talk about any possible ways we can work together!<br>
                    </p>
                  </div>
                </p>

                <p style="text-align:center">
                  <a href="https://zengyh1900.github.io/zengyh1900_cv.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="mailto:zengyh1900@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=14LbnMIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/zengyh1900">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/zengyh1900/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/zengyh1900/">Linkedin</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profiles/me-lol.gif"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profiles/me-lol.gif" class="hoverZoomLink"></a>
                <!-- <a href="images/profiles/me-angry.gif"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profiles/me-angry.gif" class="hoverZoomLink"></a> -->
                <!-- <p style="text-align: center;">status: struggle with ddl</p> -->
              </td>
            </tr>
          </tbody></table>

<!-- ============================ News ==================================== -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>News</h2>
        <div style="max-height: 250px; overflow-y: auto;">
          <ul>
            <li><b>[2025.04]</b>  ðŸ’— I have joined Ant Research to start a new journey!</li>
            <li><b>[2025.03]</b>
              ðŸŽ‰ <a href="https://jianzongwu.github.io/projects/diffsensei/">DiffSensei</a> and <a href="https://yichengchen24.github.io/projects/autocherrypicker/"> Auto-CherryPicker</a> are accepted by CVPR 2025.</li>
            <li><b>[2024.09]</b>
                ðŸŽ‰ <a href="https://humanvid.github.io/">HumanVid</a> is accepted by NeurIPS 2024 (D&B Track).</li>
            <li><b>[2024.09]</b>
                ðŸŽ‰ <a href="https://jianzongwu.github.io/projects/motionbooth/">MotionBooth</a> is accepted by NeurIPS 2024 (Spotlight).</li>
            <li><b>[2024.07]</b>
                ðŸŽ‰ <a href="https://powerpaint.github.io/">PowerPaint</a> is accepted by ECCV 2024.</li>
            <li><b>[2024.03]</b>
                ðŸŽ‰ <a href="https://pi-animator.github.io/">PIA</a> and <a href="https://make-it-vivid.github.io/">Make-it-Vivid</a> are accepted by CVPR 2024.</li>
            <li><b>[2024.02]</b>
                ðŸ”¥ Our technology has been shipped in the animation series
               <a href="https://tv.cctv.com/2024/02/26/VIDAUw4U4rxtLHnKuKP9dFZV240226.shtml">"Poems of Timeless Acclaim"</a>,
               which is broadcasted in over 10 languages and on more than 70 mainstream media platforms
               overseas. It has reached an audience of nearly 100 million worldwide viewers within two weeks.
            </li>
            <li><b>[2024.01]</b>
                ðŸ”¥ We release <a href="https://magicmaker.openxlab.org.cn/home">MagicMaker</a>,
               an AI platform that supports image generation, editing and animation!
            </li>
            <li><b>[2023.12]</b>
              We release <a href="https://github.com/open-mmlab/mmagic">MMagic</a>, a multimodal advanced, generative, and intelligent creation toolbox.
            </li>
          </ul>
        </div>
      </td>
    </tr>
    </tbody></table>


<br>
<br>
<br>
<!-- ================================================= -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Selected Publications</h2>
        <!-- <p>
        *Equal contribution. &diams; Corresponding author. <br>
        </p> -->
      </td>
    </tr>
  </tbody></table>

<!-- =====================Selected Publications============================  -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr >
      <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/DiffSensei.png' width=80%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://jianzongwu.github.io/projects/diffsensei/">
          <span class="papertitle">
              DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation
          </span>
        </a>
        <br>
        <a href="https://jianzongwu.github.io/">Jianzong Wu</a>,
        <a href="">Chao Tang</a>,
        <a href="https://wangjingbo1219.github.io/">Jingbo Wang</a>,
        <strong>Yanhong Zeng</strong>,
        <a href="https://lxtgh.github.io/">Xiangtai Li</a>,
        <a href="https://scholar.google.com/citations?user=T4gqdPkAAAAJ">Yunhai Tong</a>
        <br>
        <em>CVPR</em>, 2025
        <br>
        <a href="https://jianzongwu.github.io/projects/diffsensei/">project page</a> /
        <a href="https://arxiv.org/abs/2412.07589">arXiv</a> /
        <a href="https://huggingface.co/datasets/jianzongwu/MangaZero">dataset</a> /
        <a href="https://jianzongwu.github.io/projects/diffsensei/static/pdfs/nobel_prize.pdf">demo</a> /
        <a href="https://github.com/jianzongwu/DiffSensei">code</a>
        <img src="https://img.shields.io/github/stars/jianzongwu/DiffSensei?style=social">
        <p>
          <strong>MangaZero</strong> is a new large-scale manga dataset containing 43K manga pages and 427K annotated panels.
          <strong>DiffSensei</strong> is the first model that can generate manga images with high-quality and controllable multiple characters with complex scenes.
        </p>
      </td>
    </tr>

    <tr >
      <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/cherrypicker.png' width=80%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://yichengchen24.github.io/projects/autocherrypicker/">
          <span class="papertitle">
            Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language
          </span>
        </a>
        <br>
        <a href="https://yichengchen24.github.io/">Yicheng Cheng</a>,
        <a href="https://lxtgh.github.io/">Xiangtai Li</a>,
        <a href="https://liyn.site/">Yining Li</a>,
        <strong>Yanhong Zeng</strong>,
        <a href="https://jianzongwu.github.io/">Jianzong Wu</a>,
        <a href="https://scholar.google.com/citations?hl=zh-CN&user=eqFr7IgAAAAJ">Xiangyu Zhao</a>,
        <a href="https://chenkai.site/">Kai Chen</a>
        <br>
        <em>CVPR</em>, 2025
        <br>
        <a href="https://yichengchen24.github.io/projects/autocherrypicker/">project page</a> /
        <a href="https://arxiv.org/abs/2406.20085">arXiv</a> /
        <a href="https://github.com/yichengchen24/ACP">code</a> /
        <img src="https://img.shields.io/github/stars/yichengchen24/ACP?style=social">
        <p>
          Auto Cherry-Picker is designed to synthesize training samples for both perception and multi-modal reasoning tasks from a simple object list in natural language. It employs a nowly designed metric, CLIS, to ensure the quality of the synthetic data.
        </p>
      </td>
    </tr>

    <tr >
      <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/motionbooth.gif' width=80%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://jianzongwu.github.io/projects/motionbooth/">
          <span class="papertitle">MotionBooth: Motion-Aware Customized Text-to-Video Generation</span>
        </a>
        <br>
        <a href="https://jianzongwu.github.io/">Jianzong Wu</a>,
        <a href="https://lxtgh.github.io/">Xiangtai Li</a>,
        <strong>Yanhong Zeng</strong>,
        <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
        <a href="https://qianyuzqy.github.io/">Qianyu Zhou</a>,
        <a href="https://liyn.site/">Yining Li</a>,
        <a href="https://scholar.google.com/citations?user=T4gqdPkAAAAJ">Yunhai Tong</a>,
        <a href="https://chenkai.site/">Kai Chen</a>
        <br>
        <em>NeurIPS</em>, 2024 (Spotlight)
        <br>
        <a href="https://jianzongwu.github.io/projects/motionbooth/">project page</a> /
        <a href="https://youtu.be/iuH5iqLk5VQ">video</a> /
        <a href="https://youtu.be/iuH5iqLk5VQ">arXiv</a> /
        <a href="https://github.com/jianzongwu/MotionBooth">code</a>
        <img src="https://img.shields.io/github/stars/jianzongwu/MotionBooth?style=social">
        <p>
          MotionBooth is designed for animating customized subjects with precise control over both object and camera movements.
        </p>
      </td>
    </tr>

  <tr >
      <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/humanvid.gif' width=80%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://humanvid.github.io/">
          <span class="papertitle">HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</span>
        </a>
        <br>
        <a href="https://zhenzhiwang.github.io/">Zhenzhi Wang</a>,
        <a href="https://yixuanli98.github.io/">Yixuan Li</a>,
        <strong>Yanhong Zeng</strong>,
        <a href="https://guoyww.github.io/">Yuwei Guo</a>,
        <a href="">Youqing Fang</a>,
        <a href="">Wenran Liu</a>,
        <a href="https://sparkstj.github.io/">Jing Tan</a>,
        <a href="https://chenkai.site/">Kai Chen</a>,
        <a href="https://tianfan.info/">Tianfan Xue</a>,
        <a href="https://daibo.info/">Bo Dai</a>,
        <a href="Bo Dai ">Dahua Lin</a>
        <br>
        <em>NeurIPS</em>, 2024 (D&B Track)
        <br>
        <a href="https://humanvid.github.io/">project page</a> /
        <a href="https://arxiv.org/abs/2407.17438">arXiv</a> /
        <a href="https://github.com/zhenzhiwang/HumanVid">code</a>
        <img src="https://img.shields.io/github/stars/zhenzhiwang/HumanVid?style=social">
        <p>
          HumanVid is the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data.
        </p>
      </td>
    </tr>

  <tr >
  <tr >
      <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/foleycrafter.jpg' width=80%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://foleycrafter.github.io/">
          <span class="papertitle">FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds</span>
        </a>
        <br>
        <a href="https://ymzhang0319.github.io/">Yiming Zhang</a>,
        <a href="https://www.yichenggu.com/">Yicheng Gu</a>,
        <strong>Yanhong Zeng &diams;</strong>,
        <a href="https://scholar.google.com/citations?user=sVYO0GYAAAAJ&hl=en">Zhening Xing</a>,
        <a href="https://github.com/HeCheng0625">Yuancheng Wang</a>,
        <a href="https://drwuz.com/">Zhizheng Wu</a>,
        <a href="https://chenkai.site/">Kai Chen</a>
        <br>
        <em>Arxiv</em>, 2024
        <br>
        <a href="https://foleycrafter.github.io/">project page</a> /
        <a href="https://youtu.be/7m4YLrSBOv0">video</a> /
        <a href="https://arxiv.org/abs/2407.01494">arXiv</a> /
        <a href="https://huggingface.co/spaces/ymzhang319/FoleyCrafter">demo</a> /
        <a href="https://github.com/open-mmlab/foleycrafter">code</a>
        <img src="https://img.shields.io/github/stars/open-mmlab/foleycrafter?style=social">
        <p>
          FoleyCrafter is a text-based video-to-audio generation framework which can generate high-quality audios that are semantically relevant and temporally synchronized with the input videos.
        </p>
      </td>
    </tr>
    <tr >
    <tr >
      <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/live2diff.gif' width=80%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://live2diff.github.io/">
          <span class="papertitle">Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=sVYO0GYAAAAJ&hl=en">Zhening Xing</a>,
        <a href="https://people.mpi-inf.mpg.de/~gfox/">Gereon Fox</a>,
        <strong>Yanhong Zeng</strong>,
        <a href="https://xingangpan.github.io/">Xingang Pan</a>,
        <a href="https://people.mpi-inf.mpg.de/~elgharib/">Mohamed Elgharib</a>,
        <a href="https://www.mpi-inf.mpg.de/~theobalt/">KChristian Theobalt </a>,
        <a href="https://chenkai.site/">Kai Chen</a>
        <br>
        <em>Arxiv</em>, 2024
        <br>
        <a href="https://live2diff.github.io/">project page</a> /
        <a href="https://youtu.be/4w2cLRW3RX0">video</a> /
        <a href="https://arxiv.org/abs/2407.08701">arXiv</a> /
        <a href="https://huggingface.co/spaces/Leoxing/Live2Diff">demo</a> /
        <a href="https://github.com/open-mmlab/Live2Diff">code</a>
        <img src="https://img.shields.io/github/stars/open-mmlab/Live2Diff?style=social">
        <p>
          Live2Diff is the first attempt that enables uni-directional attention modeling to video diffusion models for live video steam processing, and achieves 16FPS on RTX 4090 GPU.
        </p>
      </td>
    </tr>

    <tr >
    <tr >
      <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/styleshot.png' width=80%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://styleshot.github.io/">
          <span class="papertitle">StyleShot: A SnapShot on Any Style</span>
        </a>
        <br>
        <a href="https://jeoyal.github.io/home/">Junyao Guo</a>,
        <a href="">Yanchen Liu</a>,
        <a href="https://scholar.google.com/citations?hl=zh-CN&user=6TA1oPkAAAAJ">Yanan Sun</a>,
        <a href="">Yinhao Tang</a>,
        <strong>Yanhong Zeng</strong>,
        <a href="https://chenkai.site/">Kai Chen</a>,
        <a href="https://vill-lab.github.io/">Cairong Zhao</a>,
        <br>
        <em>Arxiv</em>, 2024
        <br>
        <a href="https://styleshot.github.io/">project page</a> /
        <a href="https://www.youtube.com/watch?v=FC_LgSJD-lA&t=4s">video</a> /
        <a href="http://arxiv.org/abs/2407.01414">arXiv</a> /
        <a href="https://openxlab.org.cn/apps/detail/lianchen/StyleShot">demo</a> /
        <a href="https://github.com/open-mmlab/StyleShot">code</a>
        <img src="https://img.shields.io/github/stars/open-mmlab/styleshot?style=social">
        <p>
          StyleShot is a style transfer model that excels in text and image-driven style transferring without test-time style-tuning.
        </p>
      </td>
    </tr>
  <tr >
  <tr >
      <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
      <div class="one">
          <img src='images/powerpaint.png' width=80%>
      </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://powerpaint.github.io/">
          <span class="papertitle">A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting</span>
      </a>
      <br>
      <a href="https://github.com/zhuang2002">Junhao Zhuang</a>,
      <strong>Yanhong Zeng &diams;</strong>,
      <a href="">Wenran Liu</a>,
      <a href="https://scholar.google.com/citations?hl=en&user=fYdxi2sAAAAJ">Chun Yuan</a>,
      <a href="https://chenkai.site/">Kai Chen</a>
      <br>
      <em>ECCV</em>, 2024
      <br>
      <a href="https://powerpaint.github.io/">project page</a> /
      <a href="https://www.youtube.com/watch?v=7QsiY1JQUfg">video</a> /
      <a href="https://arxiv.org/abs/2312.03594">arXiv</a> /
      <a href="https://openxlab.org.cn/apps/detail/rangoliu/PowerPaint">demo</a> /
      <a href="https://github.com/zhuang2002/PowerPaint">code</a>
      <img src="https://img.shields.io/github/stars/zhuang2002/PowerPaint?style=social">
      <p>
          PowerPaint is the first versatile inpainting model that achieves SOTA in text-guided and shape-guided object inpainting, object removal, outpainting, etc.
      </p>
      </td>
  </tr>

<tr >
<tr >
  <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
    <div class="one">
      <img src='images/pia.gif' width=80%>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://pi-animator.github.io/">
      <span class="papertitle">PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models</span>
    </a>
    <br>
    <a href="https://ymzhang0319.github.io/">Yiming Zhang*</a>,
    <a href="https://scholar.google.com/citations?user=sVYO0GYAAAAJ&hl=en">Zhening Xing*</a>,
    <strong>Yanhong Zeng &diams;</strong>,
    <a href="">Youqing Fang</a>,
    <a href="https://chenkai.site/">Kai Chen</a>
    <br>
    <em>CVPR</em>, 2024
    <br>
    <a href="https://pi-animator.github.io/">project page</a> /
    <a href="https://www.youtube.com/watch?v=A7EsYGSLpYA">video</a> /
    <a href="https://arxiv.org/abs/2312.13964">arXiv</a> /
    <a href="https://huggingface.co/spaces/Leoxing/PIA">demo</a> /
    <a href="https://github.com/open-mmlab/PIA">code</a>
    <img src="https://img.shields.io/github/stars/open-mmlab/PIA?style=social">
    <p>
      PIA can animate any images from personalized models by text while preserving high-fidelity details and unique styles.
    </p>
  </td>
</tr>


<tr >
<tr >
    <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
      <div class="one">
        <img src='images/make-it-vivid.gif' width=80%>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://make-it-vivid.github.io/">
        <span class="papertitle">Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text</span>
      </a>
      <br>
      <a href="https://junshutang.github.io/">Junshu Tang</a>,
      <strong>Yanhong Zeng</strong>,
      <a href="">Ke Fan</a>,
      <a href="">Xuheng Wang</a>,
      <a href="https://daibo.info/">Bo Dai</a>,
      <a href="https://scholar.google.com/citations?user=yd58y_0AAAAJ&hl=en">Lizhuang Ma</a>,
      <a href="https://chenkai.site/">Kai Chen</a>
      <br>
      <em>CVPR</em>, 2024
      <br>
      <a href="https://make-it-vivid.github.io/">project page</a> /
      <a href="https://www.youtube.com/watch?v=WQ5QizLucbM">video</a> /
      <a href="https://arxiv.org/abs/2403.16897">arXiv</a> /
      <a href="https://github.com/junshutang/Make-It-Vivid">code</a>
      <img src="https://img.shields.io/github/stars/junshutang/Make-It-Vivid?style=social">
      <p>
      We present Make-it-Vivid, the first attempt that can create plausible and consistent texture in UV space for 3D biped cartoon characters from text input within few seconds.
      </p>
    </td>
  </tr>

<tr >
<tr >
  <td style="padding:10px;width:20%;vertical-align:middle;text-align: center;">
    <div class="one">
      <img src='images/aotgan.png' width=80%>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://sites.google.com/view/1900zyh/aot-gan?authuser=0">
      <span class="papertitle">Aggregated Contextual Transformations for High-Resolution Image Inpainting</span>
    </a>
    <br>
    <strong>Yanhong Zeng</strong>,
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
    <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>,
    <a href="https://www.microsoft.com/en-us/research/people/bainguo/">Baining Guo</a>
    <br>
    <em>TVCG</em>, 2023
    <br>
    <a href="https://sites.google.com/view/1900zyh/aot-gan?authuser=0">project page</a> /
    <a href="https://arxiv.org/abs/2104.01431">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=lYwFv0lPgOI">video 1</a> /
    <a href="https://www.youtube.com/watch?v=eCE8v10g7-E">video 2</a> /
    <a href="https://github.com/researchmm/AOT-GAN-for-Inpainting">code</a>
    <img src="https://img.shields.io/github/stars/researchmm/AOT-GAN-for-Inpainting?style=social">
    <p>
      In AOT-GAN, we propose aggregated contextual transformations and a novel mask-guided GAN training strategy for high-resolution image inpaining.
    </p>
  </td>
</tr>

<tr>
<tr>
  <td style="padding:10px;width:20%;vertical-align:middle">
    <a href="images/hdvila.png"><img src="images/hdvila.png" alt="hdvila" width="100%"></a>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2111.10337">
      <papertitle>Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions</papertitle>
    </a>
    <br>
    <strong>Yanhong Zeng*</strong>,
    <a href="https://hellwayxue.github.io/">Hongwei Xue*</a>,
    <a href="https://tiankaihang.github.io/">Tiankai Hang*</a>,
    <a href="https://scholar.google.com/citations?user=DuSxNqgAAAAJ">Yuchong Sun*</a>,
    <a href="https://www.microsoft.com/en-us/research/people/libei/">Bei Liu</a>,
    <a href="https://hyang0511.github.io/">Huan Yang</a>,
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
    <a href="https://www.microsoft.com/en-us/research/people/bainguo/">Baining Guo</a>
    <br>
    <em>CVPR</em>, 2022
    <br>
    <a href="https://arxiv.org/abs/2111.10337">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=DyN8ypuxX08">video</a> /
    <a href="https://github.com/microsoft/XPretrain">code</a>
    <img src="https://img.shields.io/github/stars/microsoft/XPretrain?style=social">
    <p>We collect a large dataset which is the first high-resolution dataset including 371.5k hours of 720p videos and the most diversified dataset covering 15 popular YouTube categories. </p>
  </td>
</tr>

<tr>
<tr>
  <td style="padding:10px;width:20%;vertical-align:middle">
    <a href="images/hdvila.png"><img src="images/tokengan.jpg" width="80%"></a>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2111.03481">
      <papertitle>Improving Visual Quality of Image Synthesis by A Token-based Generator with Transformers</papertitle>
    </a>
    <br>
    <strong>Yanhong Zeng</strong>,
    <a href="https://hyang0511.github.io/">Huan Yang</a>,
    <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>,
    <a href="https://scholar.google.com/citations?user=ZFKybdUAAAAJ&hl=en">Jianbo Wang</a>,
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>
    <br>
    <em>NeurIPS</em>, 2021
    <br>
    <a href="https://arxiv.org/abs/2111.03481">arXiv</a>
    <p>We propose a token-based generator with Transformers for image synthesis. We present a new perspective by viewing this task as visual token generation, controlled by style tokens.</p>
  </td>
</tr>

<tr>
<tr>
  <td style="padding:10px;width:20%;vertical-align:middle">
    <a href="images/sttn.jpg"><img src="images/sttn.jpg" width="80%"></a>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://sites.google.com/view/1900zyh/sttn">
      <papertitle>Learning Joint Spatial-Temporal Transformations for Video Inpainting</papertitle>
    </a>
    <br>
    <strong>Yanhong Zeng</strong>,
    <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>,
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>
    <br>
    <em>ECCV</em>, 2020
    <br>
    <a href="https://sites.google.com/view/1900zyh/sttn">project page</a> /
    <a href="https://arxiv.org/abs/2007.10247">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=X_jD4vsuOJc">video 1 </a> /
    <a href="https://www.youtube.com/watch?v=tgiWGdr1SnE">more results</a> /
    <a href="https://github.com/researchmm/STTN">code</a>
    <img src="https://img.shields.io/github/stars/researchmm/STTN?style=social">
    <p>We propose STTN, the first transformer-based model for high-quality image inpainting, setting a new state-of-the-art performance.</p>
  </td>
</tr>

<tr>
<tr>
  <td style="padding:10px;width:20%;vertical-align:middle">
    <a href="images/pennet.gif"><img src="images/pennet.gif" width="80%"></a>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://sites.google.com/view/1900zyh/pen-net?authuser=0">
      <papertitle>Learning Pyramid Context-Encoder Network for High-Quality Image Inpainting</papertitle>
    </a>
    <br>
    <strong>Yanhong Zeng</strong>,
    <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>,
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
    <a href="https://www.microsoft.com/en-us/research/people/bainguo/">Baining Guo</a>
    <br>
    <em>CVPR</em>, 2019
    <br>
    <a href="https://sites.google.com/view/1900zyh/pen-net?authuser=0">project page</a> /
    <a href="https://arxiv.org/abs/1904.07475">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=Tx2lBGybfNA">video</a> /
    <a href="https://github.com/researchmm/PEN-Net-for-Inpainting">code</a>
    <img src="https://img.shields.io/github/stars/researchmm/PEN-Net-for-Inpainting?style=social">
    <p>We propose PEN-Net, the first work that is able to conduct both semantic and texture inpainting. To achieve this, we propose cross-layer attention transfer and pyramid filling strategy.</p>
  </td>
</tr>
<tr>
<tr>
  <td style="padding:10px;width:20%;vertical-align:middle">
    <a href="images/3dhumanbody.png"><img src="images/3dhumanbody.png" width="80%"></a>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://sites.google.com/view/1900zyh/3dhumanbody?authuser=0">
      <papertitle>3D Human Body Reshaping with Anthropometric Modeling</papertitle>
    </a>
    <br>
    <strong>Yanhong Zeng</strong>,
    <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>,
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>
    <br>
    <em>ICIMCS</em>, 2017
    <br>
    <a href="https://sites.google.com/view/1900zyh/3dhumanbody?authuser=0">project page</a> /
    <a href="https://arxiv.org/abs/2104.01762">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=s0GvkER-Y24">video</a> /
    <a href="https://github.com/zengyh1900/3D-Human-Body-Shape">code</a>
    <img src="https://img.shields.io/github/stars/zengyh1900/3D-Human-Body-Shape?style=social">
    <p>We design a 3D human body reshaping system. It can take as input user's anthropometric measurements (e.g., height and weight) and generate a 3D human shape for the user.</p>
  </td>
</tr>

</tbody></table>

<!-- =================== working ============================== -->


<br>
<br>
<br>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <h2>Working Experience</h2>
    </td>
  </tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="10"><tbody>
  <tr>
    <td style="padding:5px;width:25%;vertical-align:middle">
      <img src="images/antgroup.png" style="width:100%;text-align: center;" ></td>
    <td width="75%" valign="center">
      <a href="https://www.antresearch.com/">Ant Group</a>
      <p>Researcher, 2025.04 ~ present</p>
    </td>
  </tr>
  <tr>
    <td style="padding:5px;width:25%;vertical-align:middle">
      <img src="images/shailab.jpg" style="width:90%;text-align: center;" ></td>
    <td width="75%" valign="center">
      <a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a>
      <p>Researcher, 2022.07 ~ 2025.03</p>
    </td>
  </tr>
  <tr>
    <td style="padding:5px;width:25%;vertical-align:middle">
      <img src="images/microsoft.png"  style="width:90%;text-align: center;"></td>
    <td width="75%" valign="center">
      <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia (MSRA)</a>
      <p>Research Intern, 2018.06 ~ 2021.12</p>
      <p>Research Intern, 2016.06 ~ 2017.06</p>
    </td>
  </tr>
</tbody></table>




<!-- ====================== Projects =========================== -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
      <tr>
        <td>
          <h2>Projects</h2>
        </td>
      </tr>
    </tbody></table>
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/magicmaker.png" style="width:100%;text-align: center;" ></td>
        <td width="75%" valign="center">
          <a href="https://magicmaker.openxlab.org.cn/home">MagicMaker</a>
          <p>Project Owner, 2023.04 ~ 2024.09</p>
          MagicMaker is a user-friendly AI platform that enables seamless image generation, editing, and animation. It empowers users to transform their imagination into captivating cinema and animations with ease.
          <br>
        </td>
      </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmagic.png" width="180px"></td>
              <td width="75%" valign="center">
                <a href="https://github.com/open-mmlab/mmagic">OpenMMLab/MMagic</a>
                <img src="https://img.shields.io/github/stars/open-mmlab/MMagic?style=social">
                <p>Lead Core Maintainer, 2022.07 ~ 2023.08</p>
                OpenMMLab Multimodal Advanced, Generative, and Intelligent Creation Toolbox. Unlock the magic ðŸª„: Generative-AI (AIGC), easy-to-use APIs, awesome model zoo, diffusion models, for text-to-image generation, image/video restoration/enhancement, etc.
                <br>
              </td>
            </tr>

        </tbody></table>



<!-- ===================== MISC ============================ -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
      <tr>
        <td>
          <h2>Miscellanea</h2>
        </td>
      </tr>
    </tbody></table>
    <table width="100%" align="center" border="0" cellpadding="10"><tbody>
      <ul>
        <li><b>Conference Reviewer:</b>
          CVPR, ICCV, ECCV, SIGGRAPH, ICML, ICLR, NeurIPS, AAAI.
        </li>
        <li><b>Journal Reviewer:</b>
           TIP, TVCG, TMM, TCSVT, PR.
        </li>
        <li><b> Tutorial Talk (ICCV 2023):</b>
          <a href="https://www.bilibili.com/video/BV1P94y187HE/?spm_id_from=333.999.0.0&vd_source=4a2f786f73e4f84c86d5a2e1db9d6e97">
            MMagic: Multimodal Advanced, Generative and Intelligent Creation
            </a>
          </li>
        <li><b> Tutorial Talk (CVPR 2023):</b>
          <a href="https://www.bilibili.com/video/BV1nX4y1q7DF/?spm_id_from=333.999.0.0&vd_source=4a2f786f73e4f84c86d5a2e1db9d6e97">Learning to Generate, Edit, and Enhance Images and Videos with MMagic</a>
        </li>
        <li><b>Invited Talk:</b>
          Towards High-Quality Image Inpainting (<a href="https://www.bilibili.com/video/BV1YJ411E7et/?spm_id_from=333.999.0.0">Microsoft China Video Center on Bilibili Live 2019</a>)
        </li>
        <li>  <b>Award:</b>
          ICML 2022 Outstanding Reviewer. </li>
        <li> <b>Award:</b>
          National Scholarship in 2021 (Top 1% in SYSU). </li>
        <li>  <b>Award:</b>
          Outstanding Undergraduate Thesis in 2017.</li>
        <li>  <b>Award:</b>
          Outstanding Undergraduate in 2017.</li>
        <li>  <b>Award:</b>
          National Scholarship in 2016 (Top 1% in SYSU).</li>
        <li>  <b>Award:</b>
          First Prize Excellence Scholarship in 2013, 2014, 2015.</li>
        </ul>

  </tbody></table>
        </td>
      </tr>
    </table>


<!-- ===================== Acknowledge ============================ -->

<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="width:0%;vertical-align:middle">
        </td>
        <td style="width:100%;vertical-align:middle">
        <hr style="margin-top:0px">
            <p><font color="#999999">The website template was adapted from <a href="https://jonbarron.info/">Jon Barron</a>.</font></p>
        </td>
    </tr>
  </tbody>
</table>


</body>
</html>
